{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import preprocess, get_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_COUNTS = 20\n",
    "MAX_COUNTS = 1800\n",
    "# words with count < MIN_COUNTS\n",
    "# and count > MAX_COUNTS\n",
    "# will be removed\n",
    "\n",
    "MIN_LENGTH = 15\n",
    "# minimum document length \n",
    "# (number of words)\n",
    "# after preprocessing\n",
    "\n",
    "# half the size of the context around a word\n",
    "HALF_WINDOW_SIZE = 5\n",
    "# it must be that 2*HALF_WINDOW_SIZE < MIN_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "docs = dataset['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store an index with a document\n",
    "docs = [(i, doc) for i, doc in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dataset and create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:47<00:00, 397.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed short documents: 3985\n",
      "total number of tokens: 1439861\n",
      "number of tokens to be removed: 393091\n",
      "number of additionally removed short documents: 2032\n",
      "total number of tokens: 1023189\n",
      "\n",
      "minimum word count number: 14\n",
      "this number can be less than MIN_COUNTS because of document removal\n"
     ]
    }
   ],
   "source": [
    "encoded_docs, decoder, word_counts = preprocess(\n",
    "    docs, nlp, MIN_LENGTH, MIN_COUNTS, MAX_COUNTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new ids will be created for the documents.\n",
    "# create a way of restoring initial ids:\n",
    "doc_decoder = {i: doc_id for i, (doc_id, doc) in enumerate(encoded_docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12829it [00:02, 4569.99it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "# new ids are created here\n",
    "for index, (_, doc) in tqdm(enumerate(encoded_docs)):\n",
    "    windows = get_windows(doc, HALF_WINDOW_SIZE)\n",
    "    # index represents id of a document, \n",
    "    # windows is a list of (word, window around this word),\n",
    "    # where word is in the document\n",
    "    data += [[index, w[0]] + w[1] for w in windows]\n",
    "\n",
    "data = np.array(data, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a row in 'data' contains:\n",
    "# id of a document, id of a word in this document, a window around this word\n",
    "# 1 + 1 + 10\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023189"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of windows (equals to the total number of tokens)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = np.array(word_counts)\n",
    "unigram_distribution = word_counts/sum(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34min 27s, sys: 1.95 s, total: 34min 29s\n",
      "Wall time: 8min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_size = len(decoder)\n",
    "embedding_dim = 50\n",
    "\n",
    "# train a skip-gram word2vec model\n",
    "texts = [[str(j) for j in doc] for i, doc in encoded_docs]\n",
    "model = models.Word2Vec(texts, size=embedding_dim, window=5, workers=4, sg=1, negative=15, iter=70)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_vectors = np.zeros((vocab_size, embedding_dim)).astype('float32')\n",
    "for i in decoder:\n",
    "    word_vectors[i] = model.wv[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7460"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique words\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare initialization for document weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[decoder[j] for j in doc] for i, doc in encoded_docs]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 24s, sys: 1.69 s, total: 2min 26s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_topics = 25\n",
    "lda = models.LdaModel(corpus, alpha=0.9, id2word=dictionary, num_topics=n_topics)\n",
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : drug lot job ordnance school kid food young little buy\n",
      "topic 1 : atheist belief religion exist argument evidence atheism reason religious true\n",
      "topic 2 : fbi fire weapon police crime koresh batf gun place criminal\n",
      "topic 3 : card disk board mode port video software controller speed cpu\n",
      "topic 4 : team play win player season hockey league goal period score\n",
      "topic 5 : color section version character software font display type mac disk\n",
      "topic 6 : science planet earth universe theory book space shall scientific amendment\n",
      "topic 7 : modem dog signal john baltimore springfield moncton rochester radar providence\n",
      "topic 8 : cover appear book copy black green price rider bag force\n",
      "topic 9 : university history turkey armenian war muslim iran jews source press\n",
      "topic 10 : israel kill armenians armenian jews israeli leave turkish attack child\n",
      "topic 11 : jesus church bible word christian christ life sin lord love\n",
      "topic 12 : dos windows network window mouse user newsgroup microsoft driver access\n",
      "topic 13 : price sell sound offer buy sale computer apple condition monitor\n",
      "topic 14 : giz output input end sure hear order chunk max info\n",
      "topic 15 : medical patient health disease cause child report drug study age\n",
      "topic 16 : window application server widget motif user client code display version\n",
      "topic 17 : car light engine mile battery speed low oil tire brake\n",
      "topic 18 : research datum analysis center provide computer model conference scientific filter\n",
      "topic 19 : president stephanopoulos job tax money decision talk american consider congress\n",
      "topic 20 : gun bike firearm pin rate control buy wire weapon crime\n",
      "topic 21 : guy bad fire probably objective koresh claim hell gas child\n",
      "topic 22 : space launch nasa orbit satellite earth mission spacecraft shuttle moon\n",
      "topic 23 : software ftp version format graphic package datum computer display bit\n",
      "topic 24 : chip ripem encryption public security des message phone clipper pgp\n"
     ]
    }
   ],
   "source": [
    "for i, topics in lda.show_topics(n_topics, formatted=False):\n",
    "    print('topic', i, ':', ' '.join([t for t, _ in topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12829/12829 [01:17<00:00, 166.33it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_weights_init = np.zeros((len(corpus_lda), n_topics))\n",
    "for i in tqdm(range(len(corpus_lda))):\n",
    "    topics = corpus_lda[i]\n",
    "    for j, prob in topics:\n",
    "        doc_weights_init[i, j] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data.npy', data)\n",
    "np.save('word_vectors.npy', word_vectors)\n",
    "np.save('unigram_distribution.npy', unigram_distribution)\n",
    "np.save('decoder.npy', decoder)\n",
    "np.save('doc_decoder.npy', doc_decoder)\n",
    "np.save('doc_weights_init.npy', doc_weights_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
