{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = WikiCorpus('zhwiki-latest-pages-articles.xml.bz2',lemmatize=False, dictionary={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process InputQueue-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py\", line 1218, in run\n",
      "    wrapped_chunk = [list(chunk)]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/gensim/corpora/wikicorpus.py\", line 676, in <genexpr>\n",
      "    ((text, self.lemmatize, title, pageid, tokenization_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/gensim/corpora/wikicorpus.py\", line 424, in extract_pages\n",
      "    for elem in elems:\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/gensim/corpora/wikicorpus.py\", line 409, in <genexpr>\n",
      "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
      "  File \"/opt/anaconda3/lib/python3.7/xml/etree/ElementTree.py\", line 1224, in iterator\n",
      "    data = source.read(16 * 1024)\n",
      "  File \"/opt/anaconda3/lib/python3.7/bz2.py\", line 178, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/opt/anaconda3/lib/python3.7/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/opt/anaconda3/lib/python3.7/_compression.py\", line 103, in read\n",
      "    data = self._decompressor.decompress(rawblock, size)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "docs = [(i, doc) for i, doc in enumerate(wiki.get_texts())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(docs):\n",
    "    \"\"\"Tokenize, clean, and encode documents.\n",
    "\n",
    "    Arguments:\n",
    "        docs: A list of tuples (index, string), each string is a document.\n",
    "        min_length: An integer, minimum document length.\n",
    "        min_counts: An integer, minimum count of a word.\n",
    "        max_counts: An integer, maximum count of a word.\n",
    "\n",
    "    Returns:\n",
    "        encoded_docs: A list of tuples (index, list), each list is a document\n",
    "            with words encoded by integer values.\n",
    "        decoder: A dict, integer -> word.\n",
    "        word_counts: A list of integers, counts of words that are in decoder.\n",
    "            word_counts[i] is the number of occurrences of word decoder[i]\n",
    "            in all documents in docs.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_and_tokenize(doc):\n",
    "#         stopwords = codecs.open('stopwords.txt','r',encoding='utf8').readlines()\n",
    "#         stopwords = [w.strip() for w in stopwords]\n",
    "        text_j = ' '.join(doc)\n",
    "        text_s = text_j.strip()\n",
    "        seg_list = list(jieba.cut(text_s))\n",
    "#         text_last = ' '.join(seg_list)\n",
    "#         text = ' '.join(doc.split())  # remove excessive spaces\n",
    "#         text = nlp(text, tag=True, parse=False, entity=False)\n",
    "        return [t for t in seg_list if len(t) > 1]\n",
    "\n",
    "    tokenized_docs = [(i, clean_and_tokenize(doc)) for i, doc in tqdm(docs)]\n",
    "\n",
    "#     # remove short documents\n",
    "#     n_short_docs = sum(1 for i, doc in tokenized_docs if len(doc) < min_length)\n",
    "#     tokenized_docs = [(i, doc) for i, doc in tokenized_docs if len(doc) >= min_length]\n",
    "#     print('number of removed short documents:', n_short_docs)\n",
    "\n",
    "#     # remove some tokens\n",
    "#     counts = _count_unique_tokens(tokenized_docs)\n",
    "#     tokenized_docs = _remove_tokens(tokenized_docs, counts, min_counts, max_counts)\n",
    "#     n_short_docs = sum(1 for i, doc in tokenized_docs if len(doc) < min_length)\n",
    "#     tokenized_docs = [(i, doc) for i, doc in tokenized_docs if len(doc) >= min_length]\n",
    "#     print('number of additionally removed short documents:', n_short_docs)\n",
    "\n",
    "    counts = _count_unique_tokens(tokenized_docs)\n",
    "    encoder, decoder, word_counts = _create_token_encoder(counts)\n",
    "\n",
    "#     print('\\nminimum word count number:', word_counts[-1])\n",
    "#     print('this number can be less than MIN_COUNTS because of document removal')\n",
    "\n",
    "    encoded_docs = _encode(tokenized_docs, encoder)\n",
    "    return encoded_docs, decoder, word_counts\n",
    "\n",
    "\n",
    "def _count_unique_tokens(tokenized_docs):\n",
    "    tokens = []\n",
    "    for i, doc in tokenized_docs:\n",
    "        tokens += doc\n",
    "    return Counter(tokens)\n",
    "\n",
    "\n",
    "def _encode(tokenized_docs, encoder):\n",
    "    return [(i, [encoder[t] for t in doc]) for i, doc in tokenized_docs]\n",
    "\n",
    "\n",
    "def _remove_tokens(tokenized_docs, counts, min_counts, max_counts):\n",
    "    \"\"\"\n",
    "    Words with count < min_counts or count > max_counts\n",
    "    will be removed.\n",
    "    \"\"\"\n",
    "    total_tokens_count = sum(\n",
    "        count for token, count in counts.most_common()\n",
    "    )\n",
    "    print('total number of tokens:', total_tokens_count)\n",
    "\n",
    "    unknown_tokens_count = sum(\n",
    "        count for token, count in counts.most_common()\n",
    "        if count < min_counts or count > max_counts\n",
    "    )\n",
    "    print('number of tokens to be removed:', unknown_tokens_count)\n",
    "\n",
    "    keep = {}\n",
    "    for token, count in counts.most_common():\n",
    "        keep[token] = count >= min_counts and count <= max_counts\n",
    "\n",
    "    return [(i, [t for t in doc if keep[t]]) for i, doc in tokenized_docs]\n",
    "\n",
    "\n",
    "def _create_token_encoder(counts):\n",
    "\n",
    "    total_tokens_count = sum(\n",
    "        count for token, count in counts.most_common()\n",
    "    )\n",
    "    print('total number of tokens:', total_tokens_count)\n",
    "\n",
    "    encoder = {}\n",
    "    decoder = {}\n",
    "    word_counts = []\n",
    "    i = 0\n",
    "\n",
    "    for token, count in counts.most_common():\n",
    "        # counts.most_common() is in decreasing count order\n",
    "        encoder[token] = i\n",
    "        decoder[i] = token\n",
    "        word_counts.append(count)\n",
    "        i += 1\n",
    "\n",
    "    return encoder, decoder, word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do_ = docs[0]\n",
    "# # tex = ' '.join(do_)\n",
    "# # tex\n",
    "# docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/338005 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.011 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "  9%|â–Š         | 29235/338005 [07:53<1:26:50, 59.26it/s]"
     ]
    }
   ],
   "source": [
    "encoded_docs, decoder, word_counts = preprocess(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new ids will be created for the documents.\n",
    "# create a way of restoring initial ids:\n",
    "doc_decoder = {i: doc_id for i, (doc_id, doc) in enumerate(encoded_docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "HALF_WINDOW_SIZE = 5\n",
    "# new ids are created here\n",
    "for index, (_, doc) in tqdm(enumerate(encoded_docs)):\n",
    "    windows = get_windows(doc, HALF_WINDOW_SIZE)\n",
    "    # index represents id of a document, \n",
    "    # windows is a list of (word, window around this word),\n",
    "    # where word is in the document\n",
    "    data += [[index, w[0]] + w[1] for w in windows]\n",
    "\n",
    "data = np.array(data, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array(word_counts)\n",
    "unigram_distribution = word_counts/sum(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vocab_size = len(decoder)\n",
    "embedding_dim = 50\n",
    "\n",
    "# train a skip-gram word2vec model\n",
    "texts = [[str(j) for j in doc] for i, doc in encoded_docs]\n",
    "model = models.Word2Vec(texts, size=embedding_dim, window=5, workers=4, sg=1, negative=15, iter=70)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_vectors = np.zeros((vocab_size, embedding_dim)).astype('float32')\n",
    "for i in decoder:\n",
    "    word_vectors[i] = model.wv[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[decoder[j] for j in doc] for i, doc in encoded_docs]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_topics = 25\n",
    "lda = models.LdaModel(corpus, alpha=0.9, id2word=dictionary, num_topics=n_topics)\n",
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_weights_init = np.zeros((len(corpus_lda), n_topics))\n",
    "for i in tqdm(range(len(corpus_lda))):\n",
    "    topics = corpus_lda[i]\n",
    "    for j, prob in topics:\n",
    "        doc_weights_init[i, j] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data.npy', data)\n",
    "np.save('word_vectors.npy', word_vectors)\n",
    "np.save('unigram_distribution.npy', unigram_distribution)\n",
    "np.save('decoder.npy', decoder)\n",
    "np.save('doc_decoder.npy', doc_decoder)\n",
    "np.save('doc_weights_init.npy', doc_weights_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
